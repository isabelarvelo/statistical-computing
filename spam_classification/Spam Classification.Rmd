---
title: "Spam Classification Project"
author: "Isabel Arvelo"
date: '2022-05-17'
output:
  pdf_document: default
  html_document: default
---

```{r, include= FALSE}
library(stringr)
library(ggplot2)
library(waffle)
library(knitr)
library(hunspell)
library(tidyverse)
library(tm)
library(qdap)
library(tidyverse)
library(RColorBrewer)

set.seed(0)
```

```{r, include = FALSE}
stopwords <- stopwords(kind = "en")

splitMessage <- function(x) {
  firstl <- which(x == "")[1]
  header <- head(x, firstl - 1)
  body <- tail(x, length(x) - firstl)
  return (list(header = header, body = body))
}


hasAttachment <- function(x) {
  header <- x
  content_indices <- grep("content-type", header, ignore.case = TRUE)
 if (length(content_indices) == 0) {
    return (FALSE)
 }
  
  content_index = content_indices[1]
  
  if ( grepl("multipart", header[content_index],  fixed = TRUE) || grepl("MULTIPART", header[content_index],  fixed = TRUE)) {
    return (TRUE)
  }
  else return (FALSE)
}

getBoundary <- function(x) {
  if (hasAttachment(x)) {
    boundary_indices <- grep("boundary=", x,  ignore.case = TRUE)
  if (length(boundary_indices) == 0) {
    print(x)
    stop('No boundary line')
  }
    
  boundary_index = boundary_indices[1]
  
  boundary_line <- x[boundary_index]
  boundary_line <- gsub('Content.+; ', "", boundary_line)
  boundary_line <- gsub('CONTENT.+;', "", boundary_line)
  boundary_line <- gsub('content.+;', "", boundary_line)
  boundary_line <- gsub('charset.+', "", boundary_line)
  boundary_line <- gsub(".*boundary=", "", boundary_line)
  boundary_line <- gsub(".*BOUNDARY=", "", boundary_line)
  boundary_line <- gsub('^[[:space:]]+', '', boundary_line)
  boundary_line <- gsub('[[:space:]]+$', '', boundary_line)
  boundary_line <- gsub('"', "", boundary_line)
  boundary_line <- gsub(';.?type=.+$', "", boundary_line)
  boundary_line <- gsub(';', "", boundary_line)

  return(boundary_line)
  }
  
  else {
    return ("Email does not have attachments")
  }
}

extractBodyText <- function(x) {
  body <- splitMessage(x)$body
  header <- splitMessage(x)$header
  if (hasAttachment(x)) {
    boundary <- getBoundary(x)
    b_indices <- grep(boundary, body, useBytes = TRUE, fixed = TRUE)
    first_b <- b_indices[1]
    if (length(b_indices) == 0) {
      stop("No boundary")
    }
    if (length(b_indices) == 1) {
      return(tail(body, -first_b))
    }
    else {
      second_b <-b_indices[2]
      return(body[(first_b + 1):(second_b - 1)])
    }
  }
  else {
    return(body)
  }
}

extractWords <- function(x, unique) {
  body <- extractBodyText(x)
  dp_body <- unlist(lapply(body, function(x) deparse(x)))
  full_body <- paste(dp_body, collapse = ' ')
  full_body <- gsub('\\\\.{3}', ' ', full_body)
  full_body <- gsub('[[:digit:]]', ' ', full_body)
  full_body <- tolower(full_body)
  full_body <- gsub('[[:punct:]]', ' ', full_body)
  full_body <- gsub('[[:space:]][a-z][[:space:]]', ' ', full_body)
  full_body <- gsub('[[:space:]]+', ' ', full_body)
  full_body <- gsub('^[[:space:]]+', '', full_body)
  full_body <- gsub('[[:space:]]+$', '', full_body)

  words <- str_split(full_body, " ")
  if (unique) {
    return(unique(unlist(words)))
  } else {
    return(unlist(words))
  }
}

readEmailDirectory <- function(x) {
  email_files <- list.files(x, full.names = TRUE)
  emails <- lapply(email_files, readLines)
}

printBoundary <- function(directory, indices) {
  for(i in indices) {
    header <- splitMessage(directory[[i]])$header
    boundary <- getBoundary(header)
    boundaryLine <- grep("boundary=", header, value = TRUE)
    cat(boundaryLine, "\n", boundary, "\n" , "\n")
}
}
```


**Introduction**

In this project, we developed and tested a detection algorithm for spam emails. We began by downloading and exploring a corpus of over 9,000 spam and non-spam emails from the website Spam Assassin, which is an open-source spam classification software. In order to study the format of email messages, we employed text processing and manipulation to extract emails into their different components. We examined and compared different aspects of spam vs non spam emails such as the number of lines in their headers. Next, we extracted all and the unique words that appear in the body of each email message. We then implemented a supervised learning algorithm using Naive Bayesian classification based on the words in each message to calculate the strength of evidence in support of an email being spam (as compared to the evidence against the email being spam). We trained and tested the algorithm on stratified samples of real spam and non-spam email messages. After implementation, we considered different threshold values for the log Bayes Factor, and analyzed how these choices affected Type-I (false alarm) and Type-II (missed detection) error rates. I also elected to further explore trends in the subject lines and domains of sending email addresses to assess whether any additional aspects of emails could be incorporated into the classifier to improve its performance. 


**Deliverable #1** 

```{r, include = FALSE}
easy_ham <- list.files("messages/easy_ham", full.names = TRUE) 
easy_ham_2 <- list.files("messages/easy_ham_2", full.names = TRUE) 
hard_ham <- list.files("messages/hard_ham", full.names = TRUE)
spam <- list.files("messages/spam", full.names = TRUE) 
spam_2 <- list.files("messages/spam_2", full.names = TRUE) 

allham <- c(easy_ham, easy_ham_2, hard_ham)
allspam <- c(spam, spam_2)

hamEmails <- lapply(allham, readLines)
spamEmails <- lapply(allspam, readLines)

spamheaderlines <- lapply(spamEmails, function(x) length(splitMessage(x)$header) )

spamheaderlines <- unlist(spamheaderlines)
```

```{r, echo = FALSE, fig.height=3.75, fig.width=5.25}
hamheaderlines <- lapply(hamEmails, function(x) length(splitMessage(x)$header) )
hist(unlist(hamheaderlines), xlim = c(0,90), main = "Number of Lines in Headers of Ham Emails", 
     xlab = "Number of Lines", col = "lightgoldenrod1", breaks = 60, border = "lightgoldenrod1")
```

The distribution of the number of lines in the headers of ham emails is bimodal and slightly skewed left. The mean is 40.28, with a standard deviation of 16.18. The median is 45 and the middle 75% of email headers fall between 22 and 51 lines. The minimum number of lines in a header in this directory is 6 and the maximum is 91 .


```{r, echo=FALSE, fig.height=3.75, fig.width=5.25}
hist(spamheaderlines[! spamheaderlines %in% 95:600] , xlim = c(0,90) , col = "slategray1", border = "slategray1", main = "Number of Lines in Headers of Spam Emails", 
     xlab = "Number of Lines", breaks = 60)
```

The distribution of the number of lines in the headers of spam emails is unimodal and skewed right. The mean number of lines is 29.09, with a standard deviation of 13.53. The median is 26 and the middle 75% of email headers fall between 22 and 32 lines. The minimum number of lines in a header in this directory is 12 and the maximum is 412. Out of the 2,397 emails in the spam directories there are 3 emails that have headers with more than 115 lines. 

There appear to be distinct differences in the distributions of the number of lines in the headers of ham emails versus spam emails. Both distributions have similar standard deviations, but the the distribution of ham header lines are centered around higher values indicating that ham emails tend to have more lines than spam emails. However, it is important to not that there are several outliers in the spam email directory that have a high number of lines in their headers. It is definitely not deterministic, but the number of lines in an email header is a useful indicator of spam emails. 



**Deliverable #2** 

```{r, include = FALSE}
easy_ham  <- lapply(easy_ham, readLines)
easy_ham_2 <- lapply(easy_ham_2, readLines)
hard_ham <- lapply(hard_ham, readLines)
spam <- lapply(spam, readLines)
spam_2 <- lapply(spam_2, readLines)
allEmails <- c(easy_ham, easy_ham_2, hard_ham, spam, spam_2)
```

```{r, include = FALSE}
sum(unlist(lapply(hard_ham, function(x) hasAttachment(splitMessage(x)$header))))
```

1. 92 emails in the hard_ham directory have attachments.

2. In order to be confident that my getBoundary() function is working correctly, I randomly selected 25 emails from each directory and of those 25, selected the emails with attachments. I then wrote a function that extracts the lines in the header that contain "boundary=" and compared the extracted boundary line to the output of the getBoundary() function. I found that of the 15 randomly selected boundaries I manually checked, the getBoundary() function successfully extracted the boundary from the boundary line. In addition to this brute force visual check, I also randomly selected 200 emails from the all of the emails. Of those 200 random emails, 20 had attachments, and the boundary extracted from those email headers appeared later in the bodies of each of those emails. The code I used for these checks can be found on page 11 in the appendix under "Boundary Check". 

```{r, include = FALSE}
full_directory <- c(easy_ham, easy_ham_2, hard_ham, spam, spam_2)
emailsAll <- lapply(full_directory, function(x) extractWords(x, unique = TRUE) )

isSpam <- c( rep(FALSE, (length(easy_ham) + length(easy_ham_2) + length(hard_ham))) , rep(TRUE, (length(spam) + length(spam_2))))
```



**Deliverable 3**

```{r, include=FALSE}
eh_lengths <- lapply(easy_ham, function(x) {
  numwords <- length(extractWords(x, unique = FALSE))
  return(numwords)
})

eh_2_lengths <- lapply(easy_ham_2, function(x) {
  numwords <- length(extractWords(x, unique = FALSE))
  return(numwords)
})

hh_lengths <- lapply(hard_ham, function(x) {
  numwords <- length(extractWords(x, unique = FALSE))
  return(numwords)
})

spam_lengths <- lapply(spam, function(x) {
  numwords <- length(extractWords(x, unique = FALSE))
  return(numwords)
})

spam_2_lengths <- lapply(spam_2, function(x) {
  numwords <- length(extractWords(x, unique = FALSE))
  return(numwords)
})

EH <- data.frame(Directory = rep("Easy Ham", length(eh_lengths) ), Numwords = unlist(eh_lengths))

EH_2 <- data.frame(Directory = rep("Easy Ham 2", length(eh_2_lengths) ), Numwords = unlist(eh_2_lengths))

HH <- data.frame(Directory = rep("Hard Ham", length(hh_lengths) ), Numwords = unlist(hh_lengths))

S <- data.frame(Directory = rep("Spam", length(spam_lengths) ), Numwords = unlist(spam_lengths))

S2 <- data.frame(Directory = rep("Spam 2", length(spam_2_lengths) ), Numwords = unlist(spam_2_lengths))

all <- rbind(EH, EH_2, HH, S, S2)
```

```{r, echo = FALSE, fig.height=3.75, fig.width=7}
boxplot(log(all$Numwords) ~ all$Directory,
        col='steelblue',
        main='Number of Words in Each Directory',
        xlab='Directory',
        ylab='log(# of Words)', 
         cex.axis = .8) 
```

The hard ham directory generally contains emails with the most total words, as indicated by the center of its distribution that is notably greater than the medians of the other directories. The hard_ham directory also has the least outliers and is the only one that is noticeably skewed left. The two easy ham directories have a lot more outliers than the spam directories, but the distributions of log(#of words) appear to be fairly similar. 

**Deliverable 4**

```{r, include = FALSE}
ham_words <- lapply(hamEmails, function(x) {
  numwords <- length(extractWords(x, unique = TRUE))
  return(numwords)
})

spam_words <- lapply(spamEmails, function(x) {
  numwords <- length(extractWords(x, unique = TRUE))
  return(numwords)
})

ham_vec <- sort(unlist(ham_words))
spam_vec <- sort(unlist(spam_words))

ham_unique <- data.frame(Directory = rep("Ham", length(ham_vec)), Numwords = ham_vec)

spam_unique <- data.frame(Directory = rep("Spam", length(spam_vec)), Numwords = spam_vec )

all_unique <- rbind(ham_unique, spam_unique)
```

```{r, echo = FALSE, fig.height=3.75, fig.width=5.25}
plt <- ggplot(all_unique, aes(Numwords, fill = Directory)) + 
   geom_histogram(alpha = 0.5, aes(y = ..density..), position = 'identity', bins = 60) + xlab("Number of Words") + 
   xlim(0, 600)

suppressWarnings(print(plt))
```

Looking at the plot, the number of unique words appearing in an email is not a decisively clear indicator of spam, but it could be used to help identify spam emails. A reasonable threshold for flagging an email as "spam" would be about 250 unique words. Defining an outlier as any value that falls within 1.5*(IQR) above the 3rd quartile, this threshold is also consistent with the threshold for outliers for the number of unique words in the headers of ham emails, which is 242. 

**Deliverable 5**

A derivation of the Bayes Factor, representing the evidence in favor of an email being spam (relative to the evidence against it being spam), can be found attached to the end of this report. 

**Deliverable 6**

```{r, include = FALSE}
trainingFrac <- 2/3
`%!in%` <- Negate(`%in%`)

indices <- c(1: 9348)
ham_indices <- c(1: 6951)
spam_indices <- c(6952:9348)

num_Spam_training <- round(9348* 0.2564185 * trainingFrac)
num_ham_training <- round( (9348*trainingFrac) - num_Spam_training )

shuffled_spam <- sample(spam_indices, num_Spam_training)
shuffled_ham <- sample(ham_indices, num_ham_training)

training_indices <- c(shuffled_spam,shuffled_ham)
test_indices <- which(indices %!in% training_indices)


emailsTrain <- emailsAll[training_indices]
emailsTest <- emailsAll[test_indices]

isSpamTest <- unlist(lapply(test_indices, function(x) {
                      if (x %in% 1:6951) {
                        return (FALSE)
                      } else {
                        return (TRUE)
                      }
  
}))

isSpamTrain <- unlist(lapply(training_indices, function(x) {
                      if (x %in% 1:6951) {
                        return (FALSE)
                      } else {
                        return (TRUE)
                      }
  
}))
```

```{r, include=FALSE}
bow <- unique(unlist(emailsTrain))
bow <- bow[bow != ""]
```


```{r, include = FALSE}
spamTrain <- emailsTrain[which(isSpamTrain)]
hamTrain <-  emailsTrain[which(!isSpamTrain)]

CountPresentSpam <- c(rep(0, length(bow)))
CountPresentHam <- c(rep(0, length(bow)))
names(CountPresentSpam) <- bow
names(CountPresentHam ) <- bow
```

```{r, include=FALSE}
allSpamWords <- unlist(spamTrain)
spamTable <- table(allSpamWords)
spamValues <- as.vector(spamTable)
names(spamValues) <- names(spamTable)

spamOverlap <- which( names(CountPresentSpam) %in% names(spamValues) )
CountPresentSpam[spamOverlap] <- spamValues[names(CountPresentSpam[spamOverlap])]
```

```{r, include = FALSE}
allHamWords <- unlist(hamTrain)
hamTable <- table(allHamWords)
hamValues <- as.vector(hamTable)
names(hamValues) <- names(hamTable)

hamOverlap <- which( names(CountPresentHam) %in% names(hamValues) )
CountPresentHam[hamOverlap] <- hamValues[names(CountPresentHam[hamOverlap])]
```

```{r, include = FALSE}
CountAbsentHam <- c(rep(0, length(bow)))
CountAbsentSpam <- c(rep(0, length(bow)))

names(CountAbsentSpam ) <- bow
names(CountAbsentHam ) <- bow

CountAbsentHam <-  CountAbsentHam + 4634 - CountPresentHam
CountAbsentSpam <- CountAbsentSpam + 1598 - CountPresentSpam

ProbPresentSpam <- (CountPresentSpam + 0.1) / (length(spamTrain) + 0.1)
ProbAbsentSpam <- (CountAbsentSpam  + 0.1) / (length(spamTrain) + 0.1)

ProbPresentHam <- (CountPresentHam + 0.1) / (length(hamTrain) + 0.1)
ProbAbsentHam  <- (CountAbsentHam + 0.1) / (length(hamTrain) + 0.1)

logProbPresentSpam <- log(ProbPresentSpam)
logProbPresentHam <- log(ProbPresentHam)

logProbAbsentSpam <-  log(ProbAbsentSpam)  
logProbAbsentHam <- log(ProbAbsentHam)
```


```{r, include = FALSE}
ProbPresentHam['monday'] / ProbPresentSpam['monday']
ProbPresentSpam['buy'] / ProbPresentHam['buy']
```

According to the training data, the word 'monday' is 5.560457  times more likely to appear in non-Spam emails than in spam emails. The word 'buy' is 2.205774  times more likely to appear in Spam emails than in non-Spam emails. 

**Deliverable 7**

```{r, include=FALSE}
computeBF <- function(uniqueWords) {
  
  words <- uniqueWords[uniqueWords %in% bow]
  
  present_indices <- which(names(ProbPresentSpam) %in% words)
  absent_indices <- which(names(ProbPresentSpam) %!in% words)
  
  SpamPresentProbs <-  logProbPresentSpam[present_indices]
  HamPresentProbs <- logProbPresentHam[present_indices]
  
  SpamAbsentProbs <- logProbAbsentSpam[absent_indices]
  HamAbsentProbs <- logProbAbsentHam [absent_indices]
  
  spamHypothesis <- sum(SpamPresentProbs) + sum(SpamAbsentProbs)
  hamHypothesis  <- sum(HamPresentProbs) + sum(HamAbsentProbs)
  
  BF <- spamHypothesis - hamHypothesis
  
  return(BF)
}
```

```{r, include=FALSE}
testBF <- lapply(emailsTest, computeBF)

spamtestBF <- unlist(testBF[which(isSpamTest)])
nonspamtestBF <- unlist(testBF[which(!isSpamTest)])


testdf <- data.frame(BF = unlist(testBF), Category = rep(0, length(testBF)))

testdf$Category[which(isSpamTest)] <- "Spam"
testdf$Category[which(!isSpamTest)] <- "Nonspam"
```


```{r, echo = FALSE, fig.height=3.75, fig.width=5.25}
boxplot(testdf$BF ~ testdf$Category, ylim = c(-240, 375) , ylab = "log(BF)", col = "lightblue", main = "(log) Bayes Factors for Non-Spam vs Spam Emails", xlab = "Email Classification")
```

Using the plot above, a good threshold value c for classifying an emails as Spam would be 20. 

```{r, include = FALSE}
numTestSpam <- length(which(isSpamTest))
numTestHam <- length(which(!isSpamTest))
tOneVector <- c()
tTwoVector <- c()

cValues <- sort(unlist(testBF))

tOneVector <- sapply( cValues, function(c) {
  
type1num <- length(which(nonspamtestBF >= c))
typeOneRate <- ( type1num / numTestHam)

return(typeOneRate)
})

tTwoVector <- sapply( cValues, function(c) {
  
type2num <- length(which(spamtestBF < c))
typeTwoRate <- (type2num / numTestSpam )

return(typeTwoRate)
}

)
```


```{r, echo = FALSE, fig.height=3.75, fig.width=5.25}
plot(cValues,tOneVector,type="l",col="red", main = "Type I vs Type II Rates", xlab = "Threshold value", ylab = "Error Rate", xlim = c(-1000, 1000) ) 
lines(cValues,tTwoVector,col="green")
legend(1200, 0.2, legend=c("Type I", "Type II"),
       col=c("red", "green"), lty = 1,  cex=0.8)
```

Although the distinct values of the Bayes Factors for all emails in the test set range from -7588.88 to 7212.32, the error rates begin to asymptotically approach 1, so I limited the x range to give better resolution for the values with lower absolute value. 

**Deliverable 9**

a. The c value that nearly achieves  $p(Type-I Error) = p(Type-II Error)$ is c = -29.702727. 

```{r, include=FALSE}
c =  -29.702727
type1num <- length(which(nonspamtestBF >=  c))
type2num <- length(which(spamtestBF < c))

typeOneRate <- ( type1num / numTestHam)
typeTwoRate <- (type2num / numTestSpam )

typeOneRate
typeTwoRate 

typeOneRate - typeTwoRate 
```


b. The resulting type I rate and type II rates are 1.1653% and  1.251564%, respectively. 

**Deliverable 10**
```{r, include=FALSE}
tTwoVector[which(tOneVector < 0.001)[1]]
```

Enforcing a Type-I error rate of less than 0.1%, the smallest Type-II error rate that you can achieve is 0.3566959. This means that if we want to implement a classifier that is only expected to send 0.1% of non-Spam emails to spam, the probability that the classifier fails to detect a spam email is 35.67%. 

\newpage 

**Extensions**

For my extensions, I was interested in observing other properties of emails that I thought could be useful in helping classify messages as spam or non-spam. Although users themselves aren't responsible for spam classification, I wanted to look further into the first thing I typically look at when I receive an email: the subject. I wrote a function to extract the subject from the header as well as the individual words from the subject to investigate whether or not this field could be a useful parameter for my classifier. In parts of this analysis, I decided to exclude stop words. Stop words are short commonly used words and I decided to exclude these words in some parts of text processing because they are so frequently used that they carry very little information and may not be helpful in distinguishing spam vs nonspam emails. I used the list of english stopwords in the Stopwords ISO library found in the R package "stopwords". 

```{r, include=FALSE}
extractSubject <- function(x) {
  body <- splitMessage(x)$body
  header <- splitMessage(x)$header
  subjectLine <- grep("subject:", header,  ignore.case = TRUE)
  subject <- gsub("Subject: ", "", header[subjectLine])
  return(subject)
}

extractSubjectWords <- function(x, punct = FALSE, unique = FALSE) {
  subject <- extractSubject(x)
  dp_subject <- unlist(lapply(subject, function(x) deparse(x)))
  subject <- paste(dp_subject, collapse = ' ')
  subject <- tolower(subject)
  
  swords <- gsub('[[:punct:]]', '', subject)
  spunct <- gsub('[[:alpha:]]', '', subject)
  
  swords <- gsub('[[:space:]]{2,}', ' ', swords)
  spunct <- gsub('[[:space:]]{2,}', ' ', spunct)
  
  swords <- gsub('^[[:space:]]+', '', swords)
  spunct <- gsub('^[[:space:]]+', '', spunct)
  
  swords <- gsub('[[:space:]]+$', '', swords)
  spunct <- gsub('[[:space:]]+$', '', spunct)
  
  punctuation <- str_split(spunct, " ")
  words <- str_split(swords, " ")
  
  if (punct) {
    both <- c(words, punctuation)
    return(both)
  }
  
  if (unique) {
    return(unique(unlist(words)))
  }
  return (unlist(words))
  
}

stopwords <- stopwords(kind = "en")

subjectLength <- function(x) {
  subject <- extractSubject(x)
  
  length <- nchar(subject, type = 'width', keepNA = FALSE)
  return(length)
}

subjectNumWords <- function(x) {
  words <- extractSubjectWords(x)
  numWords <- length(unlist(words))
  return(numWords)
}
```

```{r, include= FALSE}
spamSubjects <- unlist(lapply(spamEmails, function(x) { extractSubjectWords(x) }))
spamSubjects <- spamSubjects[!spamSubjects %in% stopwords]

table <- table(spamSubjects)
spamSubjectWords <- as.vector(table)
names(spamSubjectWords) <- names(table)

spamSubjectsProbs <- spamSubjectWords/(sum(spamSubjectWords))

hamSubjects <- unlist(lapply(hamEmails, function(x) { extractSubjectWords(x) }))
hamSubjects <- hamSubjects[!hamSubjects %in% stopwords]

table <- table(hamSubjects)
hamSubjectWords <- as.vector(table)
names(hamSubjectWords) <- names(table)

hamSubjectsProbs <- hamSubjectWords/(sum(hamSubjectWords))

topSpamSubWords <- head(sort(spamSubjectsProbs, decreasing = TRUE), 10)
remaining <- 1 - sum(topSpamSubWords)
topSpamSubWords <- topSpamSubWords*100
topSpamSubWords 

topHamSubWords <- head(sort(hamSubjectsProbs, decreasing = TRUE), 10)
remaining <- 1 - sum(topHamSubWords)
topHamSubWords <- topHamSubWords*100
topHamSubWords 

hamSubject_df <- data.frame(words = as.character(names(spamSubjectsProbs)), freq = spamSubjectsProbs)
spamSubject_df <- data.frame(words = as.character(names(hamSubjectsProbs)), freq = hamSubjectsProbs)

merged <- hamSubject_df %>% full_join(spamSubject_df,
  by = "words")

topWordsboth = c(names(topSpamSubWords), names(topHamSubWords))

topInBoth <- merged[which(merged$words %in% topWordsboth), ]
```

The top 10 most common (non-stopword) words in the subject lines of nonspam emails were 
```{r, echo = FALSE}
kable(topHamSubWords, col.names = "Probability" )
```

The top 10 most common (non-stopword) words in the subject lines of spam emails were 
```{r, echo = FALSE}
kable(topSpamSubWords, col.names = "Probability" )
```

I then collected the 10 top most frequent (non-stopword) words in each directory and overlayed their frequency in the subjects of both directories. The gray bars represent the frequency in spam emails and the pink bars represent the frequency in ham emails. 

```{r, echo=FALSE, fig.height=3.75, fig.width=5.25}
plt <- ggplot(topInBoth, aes(x=words)) +
  geom_bar( aes(y=freq.x), position = position_dodge(), stat="identity", size=.1,  fill="slategray3", alpha=.4) + 
  geom_bar( aes(y=freq.y), position = position_dodge(), stat="identity", size=.1,  fill="plum1", alpha=.4) + 
  labs(x = "Word", y = "Frequency", title = "Words in Spam vs Ham Subject Lines") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

suppressWarnings(print(plt))
```

It is unsurprising that the words *free*, *rates*, *home*, *money*, and *get* are common in the subject of spam emails because spam emails often promise the opportunity to get free items or discounted rates on products like insurance or mortage(another common word in the subject of spam emails) to bait people to click on the message. The only words with fairly similar frequencies in both directories was *new*. I suspect that the frequency of "spammish-sounding" words like "zzzzteana" in the subject of non-spam emails originate from the hard ham directory which is closer in many respects to typical spam. 

I then proceeded to include words in the subject of the email (including stop words) in my classifier to see whether or not it improved the performance of my algorithm. I chose to weigh the words in the subject the same as the words in the body of the email. 

```{r, include = FALSE}
emailsTrainSubjects <- lapply(allEmails, function(x) extractSubjectWords(x, unique = TRUE) )

spamSubjectTrain <- emailsTrainSubjects[which(isSpamTrain)]
hamSubjectTrain <-  emailsTrainSubjects[which(!isSpamTrain)]

#bag of domains
allSubjectWords <- c(unlist(spamSubjectTrain), unlist(hamSubjectTrain))
bosw <- unique(allSubjectWords)

countSubjectSpamPresent <- c(rep(0, length(bosw)))
countSubjectHamPresent <- c(rep(0, length(bosw)))

names(countSubjectSpamPresent) <- bosw
names(countSubjectHamPresent) <- bosw


allSpamWordsSubject <- unlist(spamSubjectTrain)
spamTableSubject <- table(allSpamWordsSubject)
spamValuesSubject <- as.vector(spamTableSubject)
names(spamValuesSubject) <- names(spamTableSubject)

spamOverlapSubject<- which( names(countSubjectSpamPresent) %in% names(spamValuesSubject) )
countSubjectSpamPresent[spamOverlapSubject] <- spamValuesSubject[names(countSubjectSpamPresent[spamOverlapSubject])]

allHamWordsSubject <- unlist(hamSubjectTrain)
hamTableSubject <- table(allHamWordsSubject)
hamValuesSubject <- as.vector(hamTableSubject)
names(hamValuesSubject) <- names(hamTableSubject)

hamOverlapSubject <- which( names(countSubjectHamPresent) %in% names(hamValuesSubject) )
countSubjectHamPresent[hamOverlapSubject] <- hamValuesSubject[names(countSubjectHamPresent[hamOverlapSubject])]


CountAbsentHamSubject <- c(rep(0, length(bosw)))
CountAbsentSpamSubject <- c(rep(0, length(bosw)))

names(CountAbsentHamSubject) <- bosw
names(CountAbsentSpamSubject) <- bosw

CountAbsentHamSubject <-  CountAbsentHamSubject + 4634 - countSubjectHamPresent
CountAbsentSpamSubject  <- CountAbsentSpamSubject  + 1598 - countSubjectSpamPresent

ProbPresentSpamSubject<- (countSubjectSpamPresent  + 0.1) / (length(spamSubjectTrain) + 0.1)
ProbAbsentSpamSubject  <- (CountAbsentSpamSubject   + 0.1) / (length(spamSubjectTrain) + 0.1)

ProbPresentHamSubject <- (countSubjectHamPresent  + 0.1) / (length(hamSubjectTrain) + 0.1)
ProbAbsentHamSubject  <- (CountAbsentSpamSubject   + 0.1) / (length(hamSubjectTrain) + 0.1)

logProbPresentSpamSubject <- log(ProbPresentHamSubject)
logProbPresentHamSubject <- log(ProbAbsentHamSubject)

logProbAbsentSpamSubject <-  log(ProbAbsentSpamSubject)  
logProbAbsentHamSubject<- log(ProbAbsentHamSubject)
```

```{r, include=FALSE}
computeBF_Subject <- function(email) {
  
  uniqueWords <- extractWords(email, unique = TRUE)
  uniqueSubjectWords <- extractSubjectWords(email, unique = TRUE)
  
  words <- uniqueWords[uniqueWords %in% bow]
  subjectWords <- uniqueSubjectWords[uniqueSubjectWords %in% bosw]
  
  present_indices <- which(names(ProbPresentSpam) %in% words)
  absent_indices <- which(names(ProbPresentSpam) %!in% words)
  
  present_Subjectindices <- which(names(ProbPresentSpamSubject) %in% subjectWords)
  absent_Subjectindices <- which(names(ProbPresentSpamSubject) %!in% subjectWords)
  
  SpamPresentProbs <-  logProbPresentSpam[present_indices]
  HamPresentProbs <- logProbPresentHam[present_indices]
  
  SpamAbsentProbs <- logProbAbsentSpam[absent_indices]
  HamAbsentProbs <- logProbAbsentHam[absent_indices]
  
  SpamPresentProbsSubject <-  logProbPresentSpamSubject[present_Subjectindices]
  HamPresentProbsSubject <- logProbPresentHamSubject[absent_Subjectindices]
  
  SpamAbsentProbsSubject<- logProbAbsentSpamSubject[absent_Subjectindices]
  HamAbsentProbsSubject <- logProbAbsentHamSubject[absent_Subjectindices]
  
  spamHypothesis <- sum(SpamPresentProbs) + sum(SpamAbsentProbs) + sum(SpamPresentProbsSubject) +
    sum(SpamAbsentProbsSubject)
  hamHypothesis  <- sum(HamPresentProbs) + sum(HamAbsentProbs) + sum(HamPresentProbsSubject) +
    sum(HamAbsentProbsSubject)
    
  
  BF <- spamHypothesis - hamHypothesis
  
  return(BF)
}

```

```{r, include = FALSE}
fullEmailsTrain <- allEmails[training_indices]
fullEmailsTest <- allEmails[test_indices]

testBFSubject <- lapply(fullEmailsTest, computeBF_Subject)

numTestSpam <- length(which(isSpamTest))
numTestHam <- length(which(!isSpamTest))
tOneVector <- c()
tTwoVector <- c()

spamtestBF <- unlist(testBFSubject[which(isSpamTest)])
nonspamtestBF <- unlist(testBFSubject[which(!isSpamTest)])

cValues_Subject <- sort(unlist(testBFSubject))

tOneVector_Sub <- sapply(cValues_Subject , function(c) {
  
type1num <- length(which(nonspamtestBF >= c))
typeOneRate <- ( type1num / numTestHam)

return(typeOneRate)
})

tTwoVector_Sub <- sapply(cValues_Subject, function(c) {
  
type2num <- length(which(spamtestBF < c))
typeTwoRate <- (type2num / numTestSpam )

return(typeTwoRate)
}

)

tTwoVector_Sub[which(tOneVector_Sub < 0.001)[1]]
```

I found that including subject words in the classifier improved the performance of the classifier. By enforcing a Type-I error rate of less than 0.1%, the smallest Type-II error rate that you can achieve is 33.17%. This means that with a fixed type one error rate, including the domain in the classifier would decrease missed detections by about 2.5%. 

```{r, include=FALSE}
hamSubjectLengths <- unlist(lapply(hamEmails, function(x) { subjectNumWords(x) }))
summary(hamSubjectLengths)

spamSubjectLengths <- unlist(lapply(spamEmails, function(x) { subjectNumWords(x) }))
summary(spamSubjectLengths)
```

I then analyzed the length of subject lines between spam and non spam emails, but did not find any meaningful disparities in the distribution of subject lengths to investigate further. The mean length of ham subject lines was 5.711 with a standard deviation of 2.627 and the mean length of spam subject lines was 5.798 with a standard deviation of 2.942. 

The next element of the email header I was interested in analyzing further was the email address of the email, sending each email. I wrote functions to extract the username (eg. "robert.chambers" ), the domain (eg. "baesystems.com"), and the top level domain (eg. ".com") for each email address in the "From: " field in the header.
I decided to focus mostly on top level domains since the README file mentioned that 'Some address obfuscation has taken place, and hostnames in some cases have been replaced with "spamassassin.taint.org"'. 

```{r, include=FALSE}
extractSender <- function(x) {
  header <- splitMessage(x)$header
  senderIndex <- grep("From", header)
  
  if (length(senderIndex) < 1 ) {
    return(" ")
  }
  
  if (length(senderIndex) >= 1) {
    senderLine <- senderIndex[1]
  }
  
  sender <- gsub("^From", "", header[senderLine])
  sender <- gsub("^Received: from", "", sender)
  
  sender <- gsub("Mon.+$", "", sender)
  sender <- gsub("Tue.+$", "", sender)
  sender <- gsub("Wed.+$", "", sender)
  sender <- gsub("Thu.+$", "", sender)
  sender <- gsub("Fri.+$", "", sender)
  sender <- gsub("Sat.+$", "", sender)
  sender <- gsub("Sun.+$", "", sender)
  
  sender <- gsub("\\([^()]*\\)", "", sender)
#  sender <- gsub("//(.+//)", "", sender)
  
  sender <- gsub("^Return-Path: from", "", sender)
  
  sender <- gsub('[[:space:]]+$', '', sender)
  sender <- gsub('^[[:space:]]+', '', sender)
  
  sender <- gsub("\\([^()]*\\)", "", sender)
  sender <- gsub('>$', '', sender)
  
  return(sender)
}
```


```{r, include=FALSE}
extractUsername <- function(x) {
  sender <- extractSender(x)
  username <- gsub("@.+$", "", sender)
  return (username)
}
```

```{r, include=FALSE}
extractFullDomain <- function(x) {
  sender <- extractSender(x)
  domain <- gsub(".+@", "", sender)
  return (domain)
}
```


```{r, include=FALSE}
extractDomain <- function(x) {
  sender <- extractSender(x)
  domain <- unlist(str_split(sender, "[.]"))
  domain <- domain[length(domain)]
  
  domain <- gsub('[[:space:]]+$', '', domain)
  domain <- gsub('^[[:space:]]+', '', domain)
  
  return (domain)
}

```


```{r, include = FALSE}
spamSenders <- lapply(spamEmails, function(x) { extractSender(x)})

fullEmailsTrain <- allEmails[training_indices]
fullEmailsTest <- allEmails[test_indices]

fullSpamTrain <- fullEmailsTrain[which(isSpamTrain)]
fullHamTrain <-  fullEmailsTrain[which(!isSpamTrain)]

spamDomains <- tolower(unlist(lapply(fullSpamTrain,function(x){ extractDomain(x)})))
hamDomains <- tolower(unlist(lapply(fullHamTrain, function(x) { extractDomain(x)})))

#bag of domains
alldoms <- c(hamDomains, spamDomains)
bod <- unique(alldoms)
bod <- bod[bod != ""]

countDomainSpam <- c(rep(0, length(bod)))
countDomainHam <- c(rep(0, length(bod)))

countDomainSpamPresent <- c(rep(0, length(bod)))
countDomainHamPresent <- c(rep(0, length(bod)))

names(countDomainSpamPresent) <- bod
names(countDomainHamPresent) <- bod


allSpamDomain <- unlist(spamDomains)
spamTableDomain <- table(allSpamDomain)
spamValuesDomain <- as.vector(spamTableDomain)
names(spamValuesDomain) <- names(spamTableDomain)

spamOverlapDomain<- which( names(countDomainSpamPresent) %in% names(spamValuesDomain) )
countDomainSpamPresent[spamOverlapDomain] <- spamValuesDomain[names(countDomainSpamPresent[spamOverlapDomain])]

allHamDomain <- unlist(hamDomains)
hamTableDomain <- table(allHamDomain)
hamValuesDomain <- as.vector(hamTableDomain)
names(hamValuesDomain) <- names(hamTableDomain)

hamOverlapDomain<- which( names(countDomainHamPresent) %in% names(hamValuesDomain) )
countDomainHamPresent[hamOverlapDomain] <- hamValuesDomain[names(countDomainHamPresent[hamOverlapDomain])]
```


```{r, include=FALSE}
CountAbsentHamDomain <- c(rep(0, length(bod)))
CountAbsentSpamDomain <- c(rep(0, length(bod)))

names(CountAbsentSpamDomain) <- bod
names(CountAbsentHamDomain) <- bod

CountAbsentHamDomain <-  CountAbsentHamDomain + 4634 - countDomainHamPresent
CountAbsentSpamDomain <- CountAbsentSpamDomain + 1598 - countDomainSpamPresent

ProbPresentSpamDomain <- (countDomainSpamPresent  + 0.1) / (length(spamTrain) + 0.1)
ProbAbsentSpamDomain  <- (CountAbsentSpamDomain   + 0.1) / (length(spamTrain) + 0.1)

ProbPresentHamDomain <- (countDomainHamPresent  + 0.1) / (length(hamTrain) + 0.1)
ProbAbsentHamDomain  <- (CountAbsentHamDomain  + 0.1) / (length(hamTrain) + 0.1)

logProbPresentSpamDomain <- log(ProbPresentHamDomain)
logProbPresentHamDomain <- log(ProbAbsentHamDomain)

logProbAbsentSpamDomain <-  log(ProbAbsentSpamDomain)  
logProbAbsentHamDomain <- log(ProbAbsentHamDomain)
```

```{r, include=FALSE}
spamDomainTable <- table(spamDomains)
sDVec <- as.vector(spamDomainTable)
names(sDVec) <- names(spamDomainTable)
head(sort(sDVec, decreasing = TRUE), 10)

hamDomainTable <- table(hamDomains)
hDVec <- as.vector(hamDomainTable)


names(hDVec) <- names(hamDomainTable)
head(sort(hDVec, decreasing = TRUE), 10)

domainSpamProb <- sDVec/ length(fullSpamTrain)
domainHamProb <- hDVec/ length(fullHamTrain)

topDomainsSpam <- head(sort(domainSpamProb, decreasing = TRUE), 10)
topDomainsHam <- head(sort(domainHamProb, decreasing = TRUE), 4)

remainingSpam <- 1 - sum(topDomainsSpam)
remainingHam <- 1 -  sum(topDomainsHam)

topDomainsSpam <- c(topDomainsSpam, other = remainingSpam)
topDomainsHam <- c(topDomainsHam, other = remainingHam)

topDomainsSpam <- topDomainsSpam*100
topDomainsHam <- topDomainsHam *100
```

```{r, echo = FALSE, fig.height=3.75, fig.width=5.25}
waffle(topDomainsSpam, rows=8, size=0.6, 
       colors= brewer.pal(11, "RdYlBu"), 
       title="Domain of Spam Email Senders")
```

```{r, echo = FALSE, fig.height=3.75, fig.width=5.25}
#Domains 
waffle(topDomainsHam, rows=8, size=0.6, 
       colors=brewer.pal(5, "RdYlBu"), 
       title="Domain of Non-Spam Email Senders")
```

There are 60 unique top level domains observed in the sending email addresses in the spam directory, but only 14 unique domains in the sending emails of the non-spam directories. Sending email addresses that end with .com are the most common in both directories, but appear about 13% more frequently in the spam directory. The domain org is about 6.07 more likely amongst non-Spam emails than spam emails. 'ie', a domain in the top 4 most common domains in both spam and non spam directories is the country code top-level domain that corresponds with the ISO 3166-1 alpha-2 code for Ireland. 


```{r, include = FALSE}
computeBFT_Domain <- function(email) {
  
  uniqueWords <- extractWords(email, unique = TRUE)
  domain <- extractDomain(email)
  
  words <- uniqueWords[uniqueWords %in% bow]
  
  present_indices <- which(names(ProbPresentSpam) %in% words)
  absent_indices <- which(names(ProbPresentSpam) %!in% words)
  
  SpamPresentProbs <-  logProbPresentSpam[present_indices]
  HamPresentProbs <- logProbPresentHam[present_indices]
  
  SpamAbsentProbs <- logProbAbsentSpam[absent_indices]
  HamAbsentProbs <- logProbAbsentHam[absent_indices]
  
  present_domain <- which(names(ProbPresentSpamDomain) %in% domain)
  absent_domains <- which(names(ProbPresentSpamDomain) %!in% domain)
  
  SpamPresentProbsDomain <-  logProbPresentSpamDomain[present_domain]
  HamPresentProbsDomain <- logProbPresentHamDomain[present_domain]
  
  SpamAbsentProbsDomain <- logProbAbsentSpam[absent_domains]
  HamAbsentProbsDomain <- logProbAbsentHam[absent_domains]
  
  spamHypothesis <- sum(SpamPresentProbs) + sum(SpamAbsentProbs) + 13*(sum(SpamPresentProbsDomain) +
    sum(SpamAbsentProbsDomain))
  hamHypothesis  <- sum(HamPresentProbs) + sum(HamAbsentProbs) + 13*(sum(HamPresentProbsDomain) +
    sum(HamAbsentProbsDomain))
    
  
  BF <- spamHypothesis - hamHypothesis
  
  return(BF)
}
```


```{r, include = FALSE}
testBF_Domain<- lapply(fullEmailsTest, computeBFT_Domain)

spamtestBF_Domain<- unlist(testBF_Domain[which(isSpamTest)])
nonspamtestBF_Domain<- unlist(testBF_Domain[which(!isSpamTest)])

testBF_Domain <- data.frame(BF = unlist(testBF_Domain), Category = rep(0, length(testBF_Domain)))

testBF_Domain$Category[which(isSpamTest)] <- "Spam"
testBF_Domain$Category[which(!isSpamTest)] <- "Nonspam"

numTestSpam_D <- length(which(isSpamTest))
numTestHam_D <- length(which(!isSpamTest))
tOneVector_D <- c()
tTwoVector_D <- c()

cValues_Domain <- sort(unlist(testBF_Domain))

tOneVector_D <- sapply(cValues_Domain, function(c) {
  
type1num_D <- length(which(nonspamtestBF_Domain>= c))
typeOneRate_D <- ( type1num_D  / numTestHam_D)

return(typeOneRate_D)
})

tTwoVector_D <- sapply(cValues_Domain, function(c) {
  
type2num_D <- length(which(spamtestBF_Domain< c))
typeTwoRate_D <- (type2num_D / numTestSpam_D )

return(typeTwoRate_D)
}

)

tTwoVector_D[which(tOneVector_D < 0.001)[1]]
```

Including domain in the classifier does not improve performance. I tuned and chose the weight of this parameter by using deliverable #10 as a metric to measure the success of my classifier. Enforcing a Type-I error rate of less than 0.1%, the smallest Type-II error rate that you can achieve is 40.30%. This means that with a fixed type one error rate, including the domain in the classifier would increase missed detections by about 4.6% as compared to the original classifier. 

I then attempted to improve my extractWords function into a "deluxe version" that did not consider the previously collection of 174 stop words. I found Bayes Factor for each email only considering non-stopword words in the email and used the same metric to analyze whether or not this improved my classifier or not. 

```{r, include = FALSE}
extractWordsDeluxe <- function(x, unique) {
  body <- extractBodyText(x)
  dp_body <- unlist(lapply(body, function(x) deparse(x)))
  full_body <- paste(dp_body, collapse = ' ')
  full_body <- gsub('\\\\.{3}', ' ', full_body)
  full_body <- gsub('[[:digit:]]', ' ', full_body)
  full_body <- tolower(full_body)
  full_body <- gsub('[[:punct:]]', ' ', full_body)
  full_body <- gsub('[[:space:]][a-z][[:space:]]', ' ', full_body)
  full_body <- gsub('[[:space:]]+', ' ', full_body)
  full_body <- gsub('^[[:space:]]+', '', full_body)
  full_body <- gsub('[[:space:]]+$', '', full_body)

  words <- str_split(full_body, " ")
  
  words <- unlist(words)
  
  words <- words[!words %in% stopwords]
  
  
  if (unique) {
    return(unique(unlist(words)))
  } else {
    return(unlist(words))
  }
  
}
```


```{r, include=FALSE}
emailsAllDeluxe <- lapply(allEmails, function(x) extractWordsDeluxe(x, unique = TRUE) )

emailsTrainDeluxe <- emailsAllDeluxe[training_indices]
emailsTestDeluxe <- emailsAllDeluxe[test_indices]

bowDeluxe <- unique(unlist(emailsTrainDeluxe))
bowDeluxe<- bowDeluxe[bowDeluxe != ""]


spamTrainDeluxe <- emailsTrainDeluxe[which(isSpamTrain)]
hamTrainDeluxe <-  emailsTrainDeluxe[which(!isSpamTrain)]
```

```{r, include=FALSE}
CountPresentSpam_2 <- c(rep(0, length(bowDeluxe)))
CountPresentHam_2 <- c(rep(0, length(bowDeluxe)))
names(CountPresentSpam_2) <- bowDeluxe
names(CountPresentHam_2 ) <- bowDeluxe
```

```{r, include=FALSE}
allSpamWordsDeluxe <- unlist(spamTrainDeluxe)
spamTableDeluxe <- table(allSpamWordsDeluxe)
spamValuesDeluxe <- as.vector(spamTableDeluxe)
names(spamValuesDeluxe) <- names(spamTableDeluxe)

spamOverlapDeluxe <- which( names(CountPresentSpam_2) %in% names(spamValuesDeluxe) )
CountPresentSpam_2[spamOverlapDeluxe] <- spamValuesDeluxe[names(CountPresentSpam_2[spamOverlapDeluxe])]
```

```{r, include=FALSE}
allHamWordsDeluxe <- unlist(hamTrainDeluxe)
hamTableDeluxe <- table(allHamWordsDeluxe)
hamValuesDeluxe <- as.vector(hamTableDeluxe)
names(hamValuesDeluxe) <- names(hamTableDeluxe)

hamOverlapDeluxe <- which( names(CountPresentHam_2) %in% names(hamValuesDeluxe) )
CountPresentHam_2[hamOverlapDeluxe] <- hamValuesDeluxe[names(CountPresentHam_2[hamOverlapDeluxe])]
```


```{r, include = FALSE}
CountAbsentHamDeluxe <- c(rep(0, length(bowDeluxe)))
CountAbsentSpamDeluxe <- c(rep(0, length(bowDeluxe)))

names(CountAbsentSpamDeluxe ) <- bowDeluxe
names(CountAbsentHamDeluxe ) <- bowDeluxe

CountAbsentHamDeluxe <-  CountAbsentHamDeluxe + 4634 - CountPresentHam_2
CountAbsentSpamDeluxe <- CountAbsentSpamDeluxe + 1598 - CountPresentSpam_2

ProbPresentSpamDeluxe <- (CountPresentSpam_2 + 0.1) / (length(spamTrainDeluxe) + 0.1)
ProbAbsentSpamDeluxe <- (CountAbsentSpamDeluxe  + 0.1) / (length(spamTrainDeluxe) + 0.1)

ProbPresentHamDeluxe <- (CountPresentHam_2 + 0.1) / (length(hamTrainDeluxe) + 0.1)
ProbAbsentHamDeluxe  <- (CountAbsentHamDeluxe + 0.1) / (length(hamTrainDeluxe) + 0.1)

logProbPresentSpamDeluxe <- log(ProbPresentSpamDeluxe)
logProbPresentHamDeluxe <- log(ProbPresentHamDeluxe)

logProbAbsentSpamDeluxe <-  log(ProbAbsentSpamDeluxe)  
logProbAbsentHamDeluxe <- log(ProbAbsentHamDeluxe)

```

```{r, include = FALSE}
computeBFT_Three <- function(email) {
  
  uniqueWords <- extractWordsDeluxe(email, unique = FALSE)
  domain <- extractDomain(email)
  
  words <- uniqueWords[uniqueWords %in% bowDeluxe]
  
  present_indices <- which(names(ProbPresentSpamDeluxe) %in% words)
  absent_indices <- which(names(ProbPresentSpamDeluxe) %!in% words)
  
  SpamPresentProbs <-  logProbPresentSpamDeluxe[present_indices]
  HamPresentProbs <- logProbPresentHamDeluxe[present_indices]
  
  SpamAbsentProbs <- logProbAbsentSpamDeluxe[absent_indices]
  HamAbsentProbs <- logProbAbsentHamDeluxe[absent_indices]
  
  spamHypothesis <- sum(SpamPresentProbs) + sum(SpamAbsentProbs) 
  hamHypothesis  <- sum(HamPresentProbs) + sum(HamAbsentProbs)
  
  BF <- spamHypothesis - hamHypothesis
  
  return(BF)
}
```


```{r, include=FALSE}
testBF_3 <- lapply(fullEmailsTest, computeBFT_Three)
```


```{r, include=FALSE}
spamtestBF_3 <- unlist(testBF_3[which(isSpamTest)])
nonspamtestBF_3 <- unlist(testBF_3[which(!isSpamTest)])

numTestSpam_3 <- length(which(isSpamTest))
numTestHam_3 <- length(which(!isSpamTest))
tOneVector_3 <- c()
tTwoVector_3 <- c()

cValues_3 <- sort(unlist(testBF_3))

tOneVector_3 <- sapply( cValues_3, function(c) {
  
type1num_3 <- length(which(nonspamtestBF_3 >= c))
typeOneRate_3 <- ( type1num_3  / numTestHam_3 )

return(typeOneRate_3)
})

tTwoVector_3 <- sapply( cValues_3, function(c) {
  
type2num_3 <- length(which(spamtestBF_3 < c))
typeTwoRate_3 <- (type2num_3 / numTestSpam_3)

return(typeTwoRate_3)
}

)

tTwoVector_3[which(tOneVector_3 < 0.001)][1]
```

The extractWordsDeluxe function, only considering words not in the stopwords list, made a negligible difference in the performance of the classifier. This is not surprising since the words probably appeared at similar frequencies in both spam and non-spam emails so eliminating them did not make a big difference. Enforcing a Type-I error rate of less than 0.1%, the smallest Type-II error rate that you can achieve is 35.9199%. This means that with a fixed type one error rate, excluding stop words from the text processing would increase increase detections by about .2499%, as compared to the original algorithm. 

\newpage 

**Appendix:** 

```{r}
splitMessage <- function(x) {
  firstl <- which(x == "")[1]
  header <- head(x, firstl - 1)
  body <- tail(x, length(x) - firstl)
  return (list(header = header, body = body))
}
```

```{r}
hasAttachment <- function(x) {
  header <- x
  content_indices <- grep("content-type", header, ignore.case = TRUE)
 if (length(content_indices) == 0) {
    return (FALSE)
 }
  
  content_index = content_indices[1]
  
  if ( grepl("multipart", header[content_index],  fixed = TRUE) || grepl("MULTIPART", header[content_index],  fixed = TRUE)) {
    return (TRUE)
  }
  else return (FALSE)
}
```

```{r}
getBoundary <- function(x) {
  if (hasAttachment(x)) {
    boundary_indices <- grep("boundary=", x,  ignore.case = TRUE)
  if (length(boundary_indices) == 0) {
    print(x)
    stop('No boundary line')
  }
    
  boundary_index = boundary_indices[1]
  
  boundary_line <- x[boundary_index]
  boundary_line <- gsub('Content.+; ', "", boundary_line)
  boundary_line <- gsub('CONTENT.+;', "", boundary_line)
  boundary_line <- gsub('content.+;', "", boundary_line)
  boundary_line <- gsub('charset.+', "", boundary_line)
  boundary_line <- gsub(".*boundary=", "", boundary_line)
  boundary_line <- gsub(".*BOUNDARY=", "", boundary_line)
  boundary_line <- gsub('^[[:space:]]+', '', boundary_line)
  boundary_line <- gsub('[[:space:]]+$', '', boundary_line)
  boundary_line <- gsub('"', "", boundary_line)
  boundary_line <- gsub(';.?type=.+$', "", boundary_line)
  boundary_line <- gsub(';', "", boundary_line)

  return(boundary_line)
  }
  
  else {
    return ("Email does not have attachments")
  }
}
```

\newpage 

*Boundary Check*: 

```{r}
printBoundary <- function(directory, indices) {
  for(i in indices) {
    header <- splitMessage(directory[[i]])$header
    boundary <- getBoundary(header)
    boundaryLine <- grep("boundary=", header, value = TRUE)
    cat(boundaryLine, "\n", boundary, "\n" , "\n")
}
}
```

```{r, echo = FALSE}
eh_sample <- sample.int(5051, 25)
eh_Headers <- sapply(eh_sample, function(x) splitMessage(easy_ham[[x]])$header)
eh_HA <- sapply(eh_Headers, function(x) hasAttachment(x))
eh_WHA <- eh_sample[ which(eh_HA)]
if (length(eh_WHA) > 0 ) {
  random_emails <- c(eh_WHA)
  printBoundary(easy_ham, eh_WHA)
}

eh_2_sample <- sample.int(1400,25)
eh2_Headers <- sapply(eh_2_sample, function(x) splitMessage(easy_ham_2[[x]])$header)
eh2_HA <- sapply(eh2_Headers, function(x) hasAttachment(x))
eh2_WHA <- eh_2_sample[which(eh2_HA )]
if (length(eh2_WHA) > 0 ) {
  printBoundary(easy_ham_2, eh2_WHA)
}

hh_sample <- sample.int(500, 25)
hh_Headers <- sapply(hh_sample, function(x) splitMessage(hard_ham[[x]])$header)
hh_HA <- sapply(hh_Headers, function(x) hasAttachment(x))
hh_WHA <- hh_sample[which(hh_HA)]
if (length(hh_WHA) > 0 ) {
  printBoundary(hard_ham, hh_WHA)
}

#spam
spam_sample <- sample.int(1000, 25)
spam_Headers <- sapply(spam_sample, function(x) splitMessage(spam[[x]])$header)
spam_HA <- sapply(spam_Headers, function(x) hasAttachment(x))
spam_WHA <- spam_sample[which(spam_HA)]
if (length(spam_WHA) > 0 ) {
  printBoundary(spam, spam_WHA)
}

#spam2
spam2_sample <- sample.int(1397, 25)
spam2_Headers <- sapply(spam2_sample, function(x) splitMessage(spam_2[[x]])$header)
spam2_HA <- sapply(spam2_Headers , function(x) hasAttachment(x))
spam2_WHA <- spam2_sample[which(spam2_HA )]
if (length(spam2_WHA) > 0 ) {
  printBoundary(spam_2, spam2_WHA)
}

```

```{r}
sampleIndex <- sample(1:9348, 200)
sampleEmails <- allEmails[sampleIndex]

sampleHeaders <- sapply(sampleEmails, function(x) splitMessage(x)$header)
sampleBodies <- sapply(sampleEmails, function(x) splitMessage(x)$body)

sampleHasAttachments <- sapply(sampleHeaders, function(x) hasAttachment(x))

emailIndWithAttachments <- which(sampleHasAttachments)

for(i in emailIndWithAttachments) {
    header <- sampleHeaders[[i]]
    body <- sampleBodies[[i]]
    boundary <- getBoundary(header)
    if (length(grep(boundary, body, fixed = TRUE)) < 1 ) {
      print ("Failed Boundary Extraction")
      boundaryLine <- grep("boundary=", header, value = TRUE)
      print(boundary)
      cat(boundaryLine, "\n")
    } 
}
```

```{r}
extractBodyText <- function(x) {
  body <- splitMessage(x)$body
  header <- splitMessage(x)$header
  if (hasAttachment(x)) {
    boundary <- getBoundary(x)
    b_indices <- grep(boundary, body, useBytes = TRUE, fixed = TRUE)
    first_b <- b_indices[1]
    if (length(b_indices) == 0) {
      stop("No boundary")
    }
    if (length(b_indices) == 1) {
      return(tail(body, -first_b))
    }
    else {
      second_b <-b_indices[2]
      return(body[(first_b + 1):(second_b - 1)])
    }
  }
  else {
    return(body)
  }
}
```

```{r}
extractWords <- function(x, unique) {
  body <- extractBodyText(x)
  dp_body <- unlist(lapply(body, function(x) deparse(x)))
  full_body <- paste(dp_body, collapse = ' ')
  full_body <- gsub('\\\\.{3}', ' ', full_body)
  full_body <- gsub('[[:digit:]]', ' ', full_body)
  full_body <- tolower(full_body)
  full_body <- gsub('[[:punct:]]', ' ', full_body)
  full_body <- gsub('[[:space:]][a-z][[:space:]]', ' ', full_body)
  full_body <- gsub('[[:space:]]+', ' ', full_body)
  full_body <- gsub('^[[:space:]]+', '', full_body)
  full_body <- gsub('[[:space:]]+$', '', full_body)

  words <- str_split(full_body, " ")
  
  if (unique) {
    return(unique(unlist(words)))
  } else {
    return(unlist(words))
  }
}
```

```{r}
readEmailDirectory <- function(x) {
  email_files <- list.files(x, full.names = TRUE)
  emails <- lapply(email_files, readLines)
}
```

```{r}
full_directory <- c(easy_ham, easy_ham_2, hard_ham, spam, spam_2)
emailsAll <- lapply(full_directory, function(x) extractWords(x, unique = TRUE)) 

isSpam <- c( rep(FALSE, (length(easy_ham) + length(easy_ham_2) + length(hard_ham))) , rep(TRUE, (length(spam) + length(spam_2))))
```

```{r}
`%!in%` <- Negate(`%in%`)

indices <- c(1: 9348)
ham_indices <- c(1: 6951)
spam_indices <- c(6952:9348)

num_Spam_training <- round(9348* 0.2564185 * trainingFrac)
num_ham_training <- round( (9348*trainingFrac) - num_Spam_training )

shuffled_spam <- sample(spam_indices, num_Spam_training)
shuffled_ham <- sample(ham_indices, num_ham_training)

training_indices <- c(shuffled_spam,shuffled_ham)
test_indices <- which(indices %!in% training_indices)


emailsTrain <- emailsAll[training_indices]
emailsTest <- emailsAll[test_indices]

isSpamTest <- unlist(lapply(test_indices, function(x) {
                      if (x %in% 1:6951) {
                        return (FALSE)
                      } else {
                        return (TRUE)
                      }
  
}))

isSpamTrain <- unlist(lapply(training_indices, function(x) {
                      if (x %in% 1:6951) {
                        return (FALSE)
                      } else {
                        return (TRUE)
                      }
  
}))

c(mean(isSpam), mean(isSpamTrain), mean(isSpamTest))
```

```{r}
length(emailsTrain) / length(emailsAll)
```

```{r}
bow <- unique(unlist(emailsTrain))
bow <- bow[bow != ""]
```


```{r}
computeBF <- function(uniqueWords) {
  
  words <- uniqueWords[uniqueWords %in% bow]
  
  present_indices <- which(names(ProbPresentSpam) %in% words)
  absent_indices <- which(names(ProbPresentSpam) %!in% words)
  
  SpamPresentProbs <-  logProbPresentSpam[present_indices]
  HamPresentProbs <- logProbPresentHam[present_indices]
  
  SpamAbsentProbs <- logProbAbsentSpam[absent_indices]
  HamAbsentProbs <- logProbAbsentHam [absent_indices]
  
  spamHypothesis <- sum(SpamPresentProbs) + sum(SpamAbsentProbs)
  hamHypothesis  <- sum(HamPresentProbs) + sum(HamAbsentProbs)
  
  BF <- spamHypothesis - hamHypothesis
  
  return(BF)
}
```

```{r}
plotErrorRates <- function(tOneVector,tTwoVector, cValues, zoom = FALSE) {
  
    plot(cValues,tOneVector,type="l",col="red", main = "Type I vs Type II Rates", xlab = "Threshold value", ylab = "Error Rate")
lines(cValues,tTwoVector,col="green")
legend(500, 0.2, legend=c("Type I (false alarm)", "Type II (missed detection)"),
       col=c("red", "green"), lty = 1,  cex=0.8)

if (zoom) {
    plot(cValues,tOneVector,type="l",col="red", main = "Type I vs Type II Rates", xlab = "Threshold value", ylab = "Error Rate", xlim = c(-55, 20) )
lines(cValues,tTwoVector,col="green")
legend(500, 0.2, legend=c("Type I (false alarm)", "Type II (missed detection)"),
       col=c("red", "green"), lty = 1,  cex=0.8)

}

}
```

\newpage 
*Extension code*
```{r}
extractSubject <- function(x) {
  body <- splitMessage(x)$body
  header <- splitMessage(x)$header
  subjectLine <- grep("subject:", header,  ignore.case = TRUE)
  subject <- gsub("Subject: ", "", header[subjectLine])
  return(subject)
}
```

```{r}
extractSubjectWords <- function(x, punct = FALSE, unique = FALSE) {
  subject <- extractSubject(x)
  dp_subject <- unlist(lapply(subject, function(x) deparse(x)))
  subject <- paste(dp_subject, collapse = ' ')
  subject <- tolower(subject)
  
  swords <- gsub('[[:punct:]]', '', subject)
  spunct <- gsub('[[:alpha:]]', '', subject)
  
  swords <- gsub('[[:space:]]{2,}', ' ', swords)
  spunct <- gsub('[[:space:]]{2,}', ' ', spunct)
  
  swords <- gsub('^[[:space:]]+', '', swords)
  spunct <- gsub('^[[:space:]]+', '', spunct)
  
  swords <- gsub('[[:space:]]+$', '', swords)
  spunct <- gsub('[[:space:]]+$', '', spunct)
  
  punctuation <- str_split(spunct, " ")
  words <- str_split(swords, " ")
  
  if (punct) {
    both <- c(words, punctuation)
    return(both)
  }
  
  if (unique) {
    return(unique(unlist(words)))
  }
  return(unlist(words))
  
}
```

```{r}
stopwords <- stopwords(kind = "en")
```

```{r}
subjectLength <- function(x) {
  subject <- extractSubject(x)
  
  length <- nchar(subject, type = 'width', keepNA = FALSE)
  return(length)
}
```

```{r}
subjectNumWords <- function(x) {
  words <- extractSubjectWords(x)
  numWords <- length(unlist(words))
  return(numWords)
}
```


```{r}
hamSubjectLengths <- unlist(lapply(hamEmails, function(x) { subjectNumWords(x) }))
summary(hamSubjectLengths)

spamSubjectLengths <- unlist(lapply(spamEmails, function(x) { subjectNumWords(x) }))
summary(spamSubjectLengths)
```


```{r}
extractSender <- function(x) {
  header <- splitMessage(x)$header
  senderIndex <- grep("From", header)
  
  if (length(senderIndex) < 1 ) {
    return(" ")
  }
  
  if (length(senderIndex) >= 1) {
    senderLine <- senderIndex[1]
  }
  
  sender <- gsub("^From", "", header[senderLine])
  sender <- gsub("^Received: from", "", sender)
  
  sender <- gsub("Mon.+$", "", sender)
  sender <- gsub("Tue.+$", "", sender)
  sender <- gsub("Wed.+$", "", sender)
  sender <- gsub("Thu.+$", "", sender)
  sender <- gsub("Fri.+$", "", sender)
  sender <- gsub("Sat.+$", "", sender)
  sender <- gsub("Sun.+$", "", sender)
  
  sender <- gsub("\\([^()]*\\)", "", sender)
#  sender <- gsub("//(.+//)", "", sender)
  
  sender <- gsub("^Return-Path: from", "", sender)
  
  sender <- gsub('[[:space:]]+$', '', sender)
  sender <- gsub('^[[:space:]]+', '', sender)
  
  sender <- gsub("\\([^()]*\\)", "", sender)
  sender <- gsub('>$', '', sender)
  
  return(sender)
}
```


```{r}
extractUsername <- function(x) {
  sender <- extractSender(x)
  username <- gsub("@.+$", "", sender)
  return (username)
}
```

```{r}
extractFullDomain <- function(x) {
  sender <- extractSender(x)
  domain <- gsub(".+@", "", sender)
  return (domain)
}
```


```{r}
extractDomain <- function(x) {
  sender <- extractSender(x)
  domain <- unlist(str_split(sender, "[.]"))
  domain <- domain[length(domain)]
  
  domain <- gsub('[[:space:]]+$', '', domain)
  domain <- gsub('^[[:space:]]+', '', domain)
  
  return (domain)
}

```

```{r}
spamSenders <- lapply(spamEmails, function(x) { extractSender(x)})
```

```{r}
fullEmailsTrain <- allEmails[training_indices]
fullEmailsTest <- allEmails[test_indices]

fullSpamTrain <- fullEmailsTrain[which(isSpamTrain)]
fullHamTrain <-  fullEmailsTrain[which(!isSpamTrain)]

spamDomains <- tolower(unlist(lapply(fullSpamTrain,function(x){ extractDomain(x)})))
hamDomains <- tolower(unlist(lapply(fullHamTrain, function(x) { extractDomain(x)})))

#bag of domains
alldoms <- c(hamDomains, spamDomains)
bod <- unique(alldoms)
bod <- bod[bod != ""]

countDomainSpam <- c(rep(0, length(bod)))
countDomainHam <- c(rep(0, length(bod)))

names(countDomainSpam) <- bod
names(countDomainHam) <- bod

```


```{r}
computeBFT_Domain <- function(email) {
  
  uniqueWords <- extractWords(email, unique = TRUE)
  domain <- extractDomain(email)
  
  words <- uniqueWords[uniqueWords %in% bow]
  
  present_indices <- which(names(ProbPresentSpam) %in% words)
  absent_indices <- which(names(ProbPresentSpam) %!in% words)
  
  SpamPresentProbs <-  logProbPresentSpam[present_indices]
  HamPresentProbs <- logProbPresentHam[present_indices]
  
  SpamAbsentProbs <- logProbAbsentSpam[absent_indices]
  HamAbsentProbs <- logProbAbsentHam[absent_indices]
  
  present_domain <- which(names(ProbPresentSpamDomain) %in% domain)
  absent_domains <- which(names(ProbPresentSpamDomain) %!in% domain)
  
  SpamPresentProbsDomain <-  logProbPresentSpamDomain[present_domain]
  HamPresentProbsDomain <- logProbPresentHamDomain[present_domain]
  
  SpamAbsentProbsDomain <- logProbAbsentSpam[absent_domains]
  HamAbsentProbsDomain <- logProbAbsentHam[absent_domains]
  
  spamHypothesis <- sum(SpamPresentProbs) + sum(SpamAbsentProbs) + 4*(sum(SpamPresentProbsDomain) +
    sum(SpamAbsentProbsDomain))
  hamHypothesis  <- sum(HamPresentProbs) + sum(HamAbsentProbs) + 4*(sum(HamPresentProbsDomain) +
    sum(HamAbsentProbsDomain))
    
  
  BF <- spamHypothesis - hamHypothesis
  
  return(BF)
}

```

```{r}
emailsTrainSubjects <- lapply(allEmails, function(x) extractSubjectWords(x, unique = TRUE) )

spamSubjectTrain <- emailsTrainSubjects[which(isSpamTrain)]
hamSubjectTrain <-  emailsTrainSubjects[which(!isSpamTrain)]

#bag of domains
allSubjectWords <- c(unlist(spamSubjectTrain), unlist(hamSubjectTrain))
bosw <- unique(allSubjectWords)

countSubjectSpamPresent <- c(rep(0, length(bosw)))
countSubjectHamPresent <- c(rep(0, length(bosw)))

names(countSubjectSpamPresent) <- bosw
names(countSubjectHamPresent) <- bosw


allSpamWordsSubject <- unlist(spamSubjectTrain)
spamTableSubject <- table(allSpamWordsSubject)
spamValuesSubject <- as.vector(spamTableSubject)
names(spamValuesSubject) <- names(spamTableSubject)

spamOverlapSubject<- which( names(countSubjectSpamPresent) %in% names(spamValuesSubject) )
countSubjectSpamPresent[spamOverlapSubject] <- spamValuesSubject[names(countSubjectSpamPresent[spamOverlapSubject])]

allHamWordsSubject <- unlist(hamSubjectTrain)
hamTableSubject <- table(allHamWordsDeluxe)
hamValuesSubject <- as.vector(hamTableDeluxe)
names(hamValuesSubject) <- names(hamTableDeluxe)

hamOverlapSubject <- which( names(countSubjectHamPresent) %in% names(hamValuesSubject) )
countSubjectHamPresent[hamOverlapSubject] <- hamValuesSubject[names(countSubjectHamPresent[hamOverlapSubject])]


CountAbsentHamSubject <- c(rep(0, length(bosw)))
CountAbsentSpamSubject <- c(rep(0, length(bosw)))

names(CountAbsentHamSubject) <- bosw
names(CountAbsentSpamSubject) <- bosw

CountAbsentHamSubject <-  CountAbsentHamSubject + 4634 - countSubjectHamPresent
CountAbsentSpamSubject  <- CountAbsentSpamSubject  + 1598 - countSubjectSpamPresent

ProbPresentSpamSubject<- (countSubjectSpamPresent  + 0.1) / (length(spamSubjectTrain) + 0.1)
ProbAbsentSpamSubject  <- (CountAbsentSpamSubject   + 0.1) / (length(spamSubjectTrain) + 0.1)

ProbPresentHamSubject <- (countSubjectHamPresent  + 0.1) / (length(hamSubjectTrain) + 0.1)
ProbAbsentHamSubject  <- (CountAbsentSpamSubject   + 0.1) / (length(hamSubjectTrain) + 0.1)

logProbPresentSpamSubject <- log(ProbPresentHamSubject)
logProbPresentHamSubject <- log(ProbAbsentHamSubject)

logProbAbsentSpamSubject <-  log(ProbAbsentSpamSubject)  
logProbAbsentHamSubject<- log(ProbAbsentHamSubject)
```

```{r}
computeBF_Subject <- function(email) {
  
  uniqueWords <- extractWords(email, unique = TRUE)
  uniqueSubjectWords <- unlist(extractSubjectWords(email, unique = TRUE))
  
  words <- uniqueWords[uniqueWords %in% bow]
  subjectWords <- uniqueSubjectWords[uniqueSubjectWords %in% bosw]
  
  present_indices <- which(names(ProbPresentSpam) %in% words)
  absent_indices <- which(names(ProbPresentSpam) %!in% words)
  
  present_Subjectindices <- which(names(ProbPresentSpamSubject) %in% subjectWords)
  absent_Subjectindices <- which(names(ProbPresentSpamSubject) %!in% subjectWords)
  
  SpamPresentProbs <-  logProbPresentSpam[present_indices]
  HamPresentProbs <- logProbPresentHam[present_indices]
  
  SpamAbsentProbs <- logProbAbsentSpam[absent_indices]
  HamAbsentProbs <- logProbAbsentHam[absent_indices]
  
  SpamPresentProbsSubject <-  logProbPresentSpamSubject[present_Subjectindices]
  HamPresentProbsSubject <- logProbPresentHamSubject[absent_Subjectindices]
  
  SpamAbsentProbsSubject<- logProbAbsentSpamSubject[absent_Subjectindices]
  HamAbsentProbsSubject <- logProbAbsentHamSubject[absent_Subjectindices]
  
  spamHypothesis <- sum(SpamPresentProbs) + sum(SpamAbsentProbs) + sum(SpamPresentProbsSubject) +
    sum(SpamAbsentProbsSubject)
  hamHypothesis  <- sum(HamPresentProbs) + sum(HamAbsentProbs) + sum(HamPresentProbsSubject) +
    sum(HamAbsentProbsSubject)
    
  
  BF <- spamHypothesis - hamHypothesis
  
  return(BF)
}

```

```{r}
testBFSubject <- lapply(fullEmailsTest, computeBF_Subject)

numTestSpam <- length(which(isSpamTest))
numTestHam <- length(which(!isSpamTest))
tOneVector <- c()
tTwoVector <- c()

spamtestBF <- unlist(testBFSubject[which(isSpamTest)])
nonspamtestBF <- unlist(testBFSubject[which(!isSpamTest)])

cValues_Subject <- sort(unlist(testBFSubject))

tOneVector_Sub <- sapply(cValues_Subject , function(c) {
  
type1num <- length(which(nonspamtestBF >= c))
typeOneRate <- ( type1num / numTestHam)

return(typeOneRate)
})

tTwoVector_Sub <- sapply(cValues_Subject, function(c) {
  
type2num <- length(which(spamtestBF < c))
typeTwoRate <- (type2num / numTestSpam )

return(typeTwoRate)
}

)

tTwoVector_Sub[which(tOneVector_Sub < 0.001)[1]]
```

```{r}
spamSenders <- lapply(spamEmails, function(x) { extractSender(x)})

fullEmailsTrain <- allEmails[training_indices]
fullEmailsTest <- allEmails[test_indices]

fullSpamTrain <- fullEmailsTrain[which(isSpamTrain)]
fullHamTrain <-  fullEmailsTrain[which(!isSpamTrain)]

spamDomains <- tolower(unlist(lapply(fullSpamTrain,function(x){ extractDomain(x)})))
hamDomains <- tolower(unlist(lapply(fullHamTrain, function(x) { extractDomain(x)})))

#bag of domains
alldoms <- c(hamDomains, spamDomains)
bod <- unique(alldoms)
bod <- bod[bod != ""]

countDomainSpam <- c(rep(0, length(bod)))
countDomainHam <- c(rep(0, length(bod)))

countDomainSpamPresent <- c(rep(0, length(bod)))
countDomainHamPresent <- c(rep(0, length(bod)))

names(countDomainSpamPresent) <- bod
names(countDomainHamPresent) <- bod


allSpamDomain <- unlist(spamDomains)
spamTableDomain <- table(allSpamDomain)
spamValuesDomain <- as.vector(spamTableDomain)
names(spamValuesDomain) <- names(spamTableDomain)

spamOverlapDomain<- which( names(countDomainSpamPresent) %in% names(spamValuesDomain) )
countDomainSpamPresent[spamOverlapDomain] <- spamValuesDomain[names(countDomainSpamPresent[spamOverlapDomain])]

allHamDomain <- unlist(hamDomains)
hamTableDomain <- table(allHamDomain)
hamValuesDomain <- as.vector(hamTableDomain)
names(hamValuesDomain) <- names(hamTableDomain)

hamOverlapDomain<- which( names(countDomainHamPresent) %in% names(hamValuesDomain) )
countDomainHamPresent[hamOverlapDomain] <- hamValuesDomain[names(countDomainHamPresent[hamOverlapDomain])]
```


```{r}
CountAbsentHamDomain <- c(rep(0, length(bod)))
CountAbsentSpamDomain <- c(rep(0, length(bod)))

names(CountAbsentSpamDomain) <- bod
names(CountAbsentHamDomain) <- bod

CountAbsentHamDomain <-  CountAbsentHamDomain + 4634 - countDomainHamPresent
CountAbsentSpamDomain <- CountAbsentSpamDomain + 1598 - countDomainSpamPresent

ProbPresentSpamDomain <- (countDomainSpamPresent  + 0.1) / (length(spamTrain) + 0.1)
ProbAbsentSpamDomain  <- (CountAbsentSpamDomain   + 0.1) / (length(spamTrain) + 0.1)

ProbPresentHamDomain <- (countDomainHamPresent  + 0.1) / (length(hamTrain) + 0.1)
ProbAbsentHamDomain  <- (CountAbsentHamDomain  + 0.1) / (length(hamTrain) + 0.1)

logProbPresentSpamDomain <- log(ProbPresentHamDomain)
logProbPresentHamDomain <- log(ProbAbsentHamDomain)

logProbAbsentSpamDomain <-  log(ProbAbsentSpamDomain)  
logProbAbsentHamDomain <- log(ProbAbsentHamDomain)
```


```{r, echo=FALSE}
spamDomainTable <- table(spamDomains)
sDVec <- as.vector(spamDomainTable)
names(sDVec) <- names(spamDomainTable)
head(sort(sDVec, decreasing = TRUE), 10)

hamDomainTable <- table(hamDomains)
hDVec <- as.vector(hamDomainTable)


names(hDVec) <- names(hamDomainTable)
head(sort(hDVec, decreasing = TRUE), 10)

domainSpamProb <- sDVec/ length(fullSpamTrain)
domainHamProb <- hDVec/ length(fullHamTrain)

topDomainsSpam <- head(sort(domainSpamProb, decreasing = TRUE), 10)
topDomainsHam <- head(sort(domainHamProb, decreasing = TRUE), 4)

remainingSpam <- 1 - sum(topDomainsSpam)
remainingHam <- 1 -  sum(topDomainsHam)

topDomainsSpam <- c(topDomainsSpam, other = remainingSpam)
topDomainsHam <- c(topDomainsHam, other = remainingHam)

topDomainsSpam <- topDomainsSpam*100
topDomainsHam <- topDomainsHam *100
```



```{r, include = FALSE}
testBF_Domain<- lapply(fullEmailsTest, computeBFT_Domain)

spamtestBF_Domain<- unlist(testBF_Domain[which(isSpamTest)])
nonspamtestBF_Domain<- unlist(testBF_Domain[which(!isSpamTest)])

testBF_Domain <- data.frame(BF = unlist(testBF_Domain), Category = rep(0, length(testBF_Domain)))

testBF_Domain$Category[which(isSpamTest)] <- "Spam"
testBF_Domain$Category[which(!isSpamTest)] <- "Nonspam"

numTestSpam_D <- length(which(isSpamTest))
numTestHam_D <- length(which(!isSpamTest))
tOneVector_D <- c()
tTwoVector_D <- c()

cValues_Domain <- sort(unlist(testBF_Domain))

tOneVector_D <- sapply(cValues_Domain, function(c) {
  
type1num_D <- length(which(nonspamtestBF_Domain>= c))
typeOneRate_D <- ( type1num_D  / numTestHam_D)

return(typeOneRate_D)
})

tTwoVector_D <- sapply(cValues_Domain, function(c) {
  
type2num_D <- length(which(spamtestBF_Domain< c))
typeTwoRate_D <- (type2num_D / numTestSpam_D )

return(typeTwoRate_D)
}

)

tTwoVector_D[which(tOneVector_D < 0.001)[1]]
```

```{r}
extractWordsDeluxe <- function(x, unique) {
  body <- extractBodyText(x)
  dp_body <- unlist(lapply(body, function(x) deparse(x)))
  full_body <- paste(dp_body, collapse = ' ')
  full_body <- gsub('\\\\.{3}', ' ', full_body)
  full_body <- gsub('[[:digit:]]', ' ', full_body)
  full_body <- tolower(full_body)
  full_body <- gsub('[[:punct:]]', ' ', full_body)
  full_body <- gsub('[[:space:]][a-z][[:space:]]', ' ', full_body)
  full_body <- gsub('[[:space:]]+', ' ', full_body)
  full_body <- gsub('^[[:space:]]+', '', full_body)
  full_body <- gsub('[[:space:]]+$', '', full_body)

  words <- str_split(full_body, " ")
  
  words <- unlist(words)
  
  words <- words[!words %in% stopwords]
  
  
  if (unique) {
    return(unique(unlist(words)))
  } else {
    return(unlist(words))
  }
  
}
```

```{r}
computeBFT_Three <- function(email) {
  
  uniqueWords <- extractWordsDeluxe(email, unique = FALSE)
  domain <- extractDomain(email)
  
  words <- uniqueWords[uniqueWords %in% bowDeluxe]
  
  present_indices <- which(names(ProbPresentSpamDeluxe) %in% words)
  absent_indices <- which(names(ProbPresentSpamDeluxe) %!in% words)
  
  SpamPresentProbs <-  logProbPresentSpamDeluxe[present_indices]
  HamPresentProbs <- logProbPresentHamDeluxe[present_indices]
  
  SpamAbsentProbs <- logProbAbsentSpamDeluxe[absent_indices]
  HamAbsentProbs <- logProbAbsentHamDeluxe[absent_indices]
  
  spamHypothesis <- sum(SpamPresentProbs) + sum(SpamAbsentProbs) 
  hamHypothesis  <- sum(HamPresentProbs) + sum(HamAbsentProbs)
  
  BF <- spamHypothesis - hamHypothesis
  
  return(BF)
}
```





